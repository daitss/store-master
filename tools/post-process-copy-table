#!/usr/bin/env ruby

# It requires data be gathered from the silos service:
#
# curl -s http://silos.darchive.fcla.edu/fixity.csv > darchive.fixity.csv
# curl -s http://silos.tarchive.fcla.edu/fixity.csv > tarchive.fixity.csv
#
# That raw data takes about an hour to retrieve and looks something
# like this:
#
# "name","location","sha1","md5","size","time","status"
# "E20051012_AAAAAA","http://silos.tarchive.fcla.edu/007/data/E20051012_AAAAAA","0d37bd6cebd25d5601abe61ede71e1fcf31b35b7","85d8585997f6c40b0ab559072c78137d","1016350720","2011-04-02T12:52:52-04:00","ok"
# "E20051012_AAAAAB","http://silos.tarchive.fcla.edu/007/data/E20051012_AAAAAB","8533c8362453b75afc8c911e2b9b230445fc380c","bef2badc1234d429b34cec1879e2f0d0","958720000","2011-04-02T12:53:03-04:00","ok"
#
# The pool table should already have been populated; we have
#
# > select * from pools;
#
#  id | required |            services_location            | read_preference | basic_auth_username | basic_auth_password 
# ----+----------+-----------------------------------------+-----------------+---------------------+---------------------
#   2 | t        | http://silos.darchive.fcla.edu/services |               0 |                     | 
#   1 | t        | http://silos.tarchive.fcla.edu/services |               1 |                     | 
# (2 rows)
#
# Once we have all that, this program will populate the storemaster db; that takes about
# 2.5 hours to run.


$LOAD_PATH.unshift '/opt/web-services/sites/storemaster/current/lib'

require 'datyl/streams'
require 'datyl/logger'
require 'store-master/model'

module Streams

  Struct.new('PoolFixityRecord', :location, :sha1, :md5, :size, :timestamp, :status)


  class PoolDataStream < DataFileStream

    # initialized via an csv file:

    def initialize io
      io.rewind
      io.gets     # discard CSV header
      super(io)
    end
      
    def rewind
      @io.rewind
      @io.gets    # discard CSV header
      self
    end

    def read
      rec = CSV.parse_line(@io.gets)
      return rec.shift, Struct::PoolFixityRecord.new(*rec)
    end

  end
end  # of module Streams


def setup
  Logger.setup('CopyTableUpdate', 'storemaster.fda.fcla.edu')
  Logger.stderr 

  Daitss.setup_db('/opt/fda/etc/db.yml', 'ps_daitss_2')
end

include Daitss
include Streams

setup()


# PoolDataStream looks as so:
#
#   E20110129_CYXBHO, #<struct Struct::PoolFixityRecord 
#                         location="http://silos.darchive.fcla.edu/data/E20110129_CYXBHO.000",
#	                  sha1="ccd53fa068173b4f5e52e55e3f1e863fc0e0c201", 
#                         md5="4732518c5fe6dbeb8429cdda11d65c3d", 
#                         size="218734619", 
#                         timestamp="2011-01-29T02:43:50-05:00",
#                         status="ok">
#
# We get the data from the silo web services, see comments above.

cmp = ComparisonStream.new(PoolDataStream.new(File.open('darchive.fixity.csv')),
                           PoolDataStream.new(File.open('tarchive.fixity.csv')))

cmp.each do |ieid, v1, v2|

  if v1.nil?
    Logger.warn "#{ieid} missing from darchive: tarchive has " + v2.inspect
    if v2.status != 'ok'
      Logger.err "Only copy for #{ieid} has failed status, skipping"
      next
    end

    md5  = v2.md5
    sha1 = v2.sha1
    size = v2.size
    time = v2.timestamp

  elsif v2.nil?
    Logger.warn "#{ieid} missing from tarchive: darchive has " + v1.inspect

    if v1.status != 'ok'
      Logger.err "Only copy for #{ieid} has failed status, skipping"
      next
    end

    md5  = v1.md5
    sha1 = v1.sha1
    size = v1.size
    time = v1.timestamp
    
  else

    if v1.status != 'ok' and v2.status != 'ok'
      Logger.err "#{ieid} has no good fixity checks, skipping"
      next

    elsif v1.status != 'ok'
      Logger.warn "#{ieid} from darchive is bad, using tarchive's"


    elsif v2.status != 'ok'
      Logger.warn "#{ieid} from tarchive is bad, using darchive's"
      
    else # now we have two good fixity checks; sanity check

      if v1.md5 != v2.md5 or v1.sha1 != v2.sha1 or v1.size != v2.size
        Logger.err "#{ieid} mismatched fixities that should be OK - #{v1.inspect}  vs.  #{v2.inspect}"
        next
      end

      md5  = v1.md5
      sha1 = v1.sha1
      size = v1.size
      time = [ v1.timestamp, v2.timestamp ].max

    end

    url = "http://storemaster.fda.fcla.edu:70/packages/#{ieid}"
    record = Copy.first(:url => url, :md5 => md5)

    if not record
      Logger.err "#{ieid} has no copy record for #{url} #{md5}"
      next
    end
    
    # record.sha1 = sha1
    # record.size = size
    # record.timestamp = time

    # if not record.save
    #   Logger.err "#{ieid} - can't save copy record: "  + record.errors.full_messages.join('; ')
    #   next
    # end

    Logger.info "#{ieid} to be updated with #{sha1}, #{time}, #{size}"


  end  # of earch
end
