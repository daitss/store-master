#!/usr/bin/env ruby

# This script is provided to prime the store master DB from the
# existing silos as part of the data migration of DAITSS 1 to DAITSS
# 2; from that point on the packages should be manipulated from the
# store-master service.
#
# It requires data be gathered from the silos service:
#
# curl -s http://silos.darchive.fcla.edu/fixity.csv > darchive.fixity.csv
# curl -s http://silos.tarchive.fcla.edu/fixity.csv > tarchive.fixity.csv
#
# That raw data takes about an hour to retrieve and looks something
# like this:
#
# "name","location","sha1","md5","size","time","status"
# "E20051012_AAAAAA","http://silos.tarchive.fcla.edu/007/data/E20051012_AAAAAA","0d37bd6cebd25d5601abe61ede71e1fcf31b35b7","85d8585997f6c40b0ab559072c78137d","1016350720","2011-04-02T12:52:52-04:00","ok"
# "E20051012_AAAAAB","http://silos.tarchive.fcla.edu/007/data/E20051012_AAAAAB","8533c8362453b75afc8c911e2b9b230445fc380c","bef2badc1234d429b34cec1879e2f0d0","958720000","2011-04-02T12:53:03-04:00","ok"
#
# The pool table should already have been populated; we have
#
# > select * from pools;
#
#  id | required |            services_location            | read_preference | basic_auth_username | basic_auth_password 
# ----+----------+-----------------------------------------+-----------------+---------------------+---------------------
#   2 | t        | http://silos.darchive.fcla.edu/services |               0 |                     | 
#   1 | t        | http://silos.tarchive.fcla.edu/services |               1 |                     | 
# (2 rows)
#
# Once we have all that, this program will populate the storemaster db; that takes about
# 2.5 hours to run.


$LOAD_PATH.unshift File.join(File.dirname(__FILE__), 'lib')

require 'datyl/streams'
require 'datyl/logger'
require 'store-master/model'

module Streams

  Struct.new('PoolFixityRecord', :location, :sha1, :md5, :size, :timestamp, :status)


  class PoolDataStream < DataFileStream

    # initialized via an csv file:

    def initialize io
      io.rewind
      io.gets     # discard CSV header
      super(io)
    end
      
    def rewind
      @io.rewind
      @io.gets    # discard CSV header
      self
    end

    def read
      rec = CSV.parse_line(@io.gets)
      return rec.shift, Struct::PoolFixityRecord.new(*rec)
    end

  end
end  # of module Streams


def setup
  Logger.setup('StoreMasterMerge', 'storemaster.fda.fcla.edu')
  Logger.stderr 

  StoreMasterModel.setup_db('/opt/fda/etc/db.yml', 'production_storemaster')
end

include StoreMasterModel
include Streams

setup()

pool1 = Pool.lookup 'http://silos.darchive.fcla.edu/services'
pool2 = Pool.lookup 'http://silos.tarchive.fcla.edu/services'

# PoolDataStream looks as so:
#
#   E20110129_CYXBHO, #<struct Struct::PoolFixityRecord location="http://silos.darchive.fcla.edu/data/E20110129_CYXBHO.000", sha1="ccd53fa068173b4f5e52e55e3f1e863fc0e0c201", md5="4732518c5fe6dbeb8429cdda11d65c3d", size="218734619", timestamp="2011-01-29T02:43:50-05:00", status="ok">
#
# We get the data from the silo web services, see comments above.

cmp = ComparisonStream.new(PoolDataStream.new(File.open('darchive.fixity.csv')),
                           PoolDataStream.new(File.open('tarchive.fixity.csv')))

cmp.each do |ieid, v1, v2|

  if v1.nil?
    puts "#{ieid} missing from darchive: tarchive has " + v2.inspect
  elsif v2.nil?
    puts "#{ieid} missing from tarchive: darchive has " + v1.inspect
  else
    package = Package.create(:ieid => ieid, :name => ieid)

    cp1 = Copy.create(:store_location => v1.location, :datetime => v1.timestamp, :pool => pool1)
    cp2 = Copy.create(:store_location => v2.location, :datetime => v2.timestamp, :pool => pool2)

    package.copies << cp1
    package.copies << cp2

    if not package.save
      puts "Save error for #{ieid}"
      puts "copy 1: "   + cp1.errors.full_messages.join('; ')
      puts "copy 2: "   + cp2.errors.full_messages.join('; ')
      puts "package: "  + package.errors.full_messages.join('; ')
    end
  end
end
